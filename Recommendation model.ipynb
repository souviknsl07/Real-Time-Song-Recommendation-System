{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c632de3d-0dd3-4388-97fe-62e4eaa05d5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##To generate dummy training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dd306d9-932b-40bf-b44e-e8fc76f555b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# track_ids = spark.read.csv('s3://souvik-spark-streaming/music_data/track_ids/', inferSchema=True, header=True)\n",
    "\n",
    "# track_ids = track_ids.select(\"track_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# import random\n",
    "\n",
    "# def generate_user_data():\n",
    "#     return {\n",
    "#         'user_id': random.randint(0,5000),\n",
    "#         'track_id': random.choice(track_ids),\n",
    "#         'like': random.randint(0,1)\n",
    "#     }\n",
    "\n",
    "# user_data = [generate_user_data() for _ in range(10000)]\n",
    "\n",
    "# from pyspark.sql.types import *\n",
    "# # Step 3: Create schema\n",
    "# schema = StructType([\n",
    "#     StructField(\"user_id\", IntegerType(), False),\n",
    "#     StructField(\"track_id\", IntegerType(), False),\n",
    "#     StructField(\"like\", IntegerType(), False)\n",
    "# ])\n",
    "\n",
    "# # Step 4: Create DataFrame\n",
    "# user_df = spark.createDataFrame(user_data, schema)\n",
    "\n",
    "# # Show sample data\n",
    "# user_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ebc08bd-eb68-4244-baa4-2159a4264b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # user_data = spark.read.csv(\"s3a://souvik-spark-streaming/music_data/user_data/\", inferSchema=True, header=True)\n",
    "# # from pyspark.sql.functions import col\n",
    "\n",
    "# # Aliases for self-join\n",
    "# df1 = user_df.alias(\"a\")\n",
    "# df2 = user_df.alias(\"b\")\n",
    "\n",
    "# # Join on track_id, filter where user_ids are different\n",
    "# joined = df1.join(df2, (col(\"a.track_id\") == col(\"b.track_id\")) & (col(\"a.like\") == col(\"b.like\")) & (col(\"a.user_id\") != col(\"b.user_id\")))\n",
    "\n",
    "# # Optional: select the fields you want\n",
    "# joined.select(\"a.user_id\", \"b.user_id\", \"a.track_id\", \"a.like\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50bfb562-0c6a-4faa-9d23-9e9b263c5e36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Training the ALS model for collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81bb89b5-9edf-42d4-9b0a-b9f02824d565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.recommendation.ALS\n",
    "\n",
    "val aws_access_key = dbutils.secrets.get(scope=\"aws\", key=\"aws-access-key-id\")\n",
    "val aws_secret_key = dbutils.secrets.get(scope=\"aws\", key=\"aws-secret-access-key\")\n",
    "\n",
    "// spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", aws_access_key)\n",
    "// spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", aws_secret_key)\n",
    "// spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", aws_access_key)\n",
    "spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", aws_secret_key)\n",
    "spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "val old_user_events = spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .csv(\"s3a://souvik-spark-streaming/music_data/user_data/\")\n",
    "\n",
    "val new_user_events = spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .csv(\"s3a://souvik-dev-stage/new_user_data/\")\n",
    "\n",
    "val combined_user_events = old_user_events.union(new_user_events)\n",
    "\n",
    "//val positiveUserEvents = user_events.filter(\"like=1\")\n",
    "\n",
    "// val als = new ALS()\n",
    "//   .setUserCol(\"user_id\")\n",
    "//   .setItemCol(\"track_id\")\n",
    "//   .setRatingCol(\"like\")          // 1 = click (positive signal), 0 = no click\n",
    "//   .setImplicitPrefs(true)         // Treats ratings as confidence signals\n",
    "//   .setAlpha(40.0)                 // Confidence weight\n",
    "//   .setRank(5)\n",
    "//   .setMaxIter(10)\n",
    "//   .setRegParam(0.1)\n",
    "//   .setNonnegative(true)\n",
    "//   .setColdStartStrategy(\"drop\")\n",
    "\n",
    "val als = new ALS()\n",
    "  .setUserCol(\"user_id\")\n",
    "  .setItemCol(\"track_id\")\n",
    "  .setRatingCol(\"like\")\n",
    "  .setImplicitPrefs(true)        // Treats 'like=1' and 'like=0' as implicit feedback\n",
    "  .setAlpha(40.0)                // High confidence for implicit data (boosts 'like=1' importance)\n",
    "  .setRank(10)                   // Moderate latent factors\n",
    "  .setMaxIter(10)                // Enough iterations to converge on smaller datasets\n",
    "  .setRegParam(0.05)             // Slight regularization, effective for larger datasets\n",
    "  .setNonnegative(true)          // Non-negative factorization (good for recommendation systems)\n",
    "  .setColdStartStrategy(\"drop\")  // Drop rows with missing predictions (handle cold-start users/items)\n",
    "\n",
    "\n",
    "val ALSModel = als.fit(combined_user_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c87ec03-7b4c-4e97-8c16-bd15a30395aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "//val user_recommendations = ALSModel.recommendForAllUsers(5)\n",
    "//user_recommendations.show(false)\n",
    "ALSModel.write.overwrite().save(\"s3a://souvik-dev-stage/ALSModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b6a4d3f-32f0-4ead-8616-45010469f7fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val user_recommendations = ALSModel.recommendForAllUsers(5)\n",
    "display(user_recommendations.orderBy(\"user_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cca24a7-9f08-419f-bd90-deb20df0b21d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Creating content based filtering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7179f845-cf68-414e-b4e7-7ffad9a81dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val tracks = spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .csv(\"s3a://souvik-spark-streaming/music_data/tcc_ceds_music.csv\")\n",
    "\n",
    "val tracks_new = tracks.withColumnRenamed(\"_c0\", \"track_id\")\n",
    "val excludeCols = Set(\"track_id\", \"track_name\", \"lyrics\", \"len\", \"age\")\n",
    "val colsToConcat = tracks_new.columns.filterNot(excludeCols.contains).map(col)\n",
    "val tracks_final = tracks_new.withColumn(\"text\", concat_ws(\" \", colsToConcat: _*))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8959ef0c-c572-4b3b-a350-f1e888f9698c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"tokens\")\n",
    "\n",
    "val hashingTF = new HashingTF()\n",
    "  .setInputCol(\"tokens\")\n",
    "  .setOutputCol(\"rawFeatures\")\n",
    "  .setNumFeatures(1000)\n",
    "\n",
    "val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\")\n",
    "\n",
    "val tfidfPipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, idf))\n",
    "\n",
    "val tfidfModel = tfidfPipeline.fit(tracks_final)\n",
    "val itemsFeaturized = tfidfModel.transform(tracks_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e165709-6a5c-4f83-811d-e0dfd112bc97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "itemsFeaturized.write \n",
    "    .mode(\"overwrite\") \n",
    "    .option(\"header\", \"true\")\n",
    "    .parquet(\"s3a://souvik-dev-stage/contentModel/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6712df84-c80f-424f-aa64-6358e5a5f8a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val user_likes = user_events.select(\"user_id\", \"track_id\")\n",
    "val likedItems = user_likes.join(itemsFeaturized, \"track_id\")\n",
    "\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import breeze.linalg.{DenseVector => BDV}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val avgVectorUDF = udf((vectors: Seq[Vector]) => {\n",
    "  val breezeVecs = vectors.map(v => BDV(v.toArray))\n",
    "  val sumVec = breezeVecs.reduce(_ + _)\n",
    "  val avgVec = sumVec / breezeVecs.length.toDouble\n",
    "  Vectors.dense(avgVec.toArray)\n",
    "})\n",
    "\n",
    "val likedVectors = likedItems\n",
    "  .groupBy(\"user_id\")\n",
    "  .agg(collect_list($\"features\").as(\"featureList\"))\n",
    "\n",
    "val userProfile = likedVectors\n",
    "  .withColumn(\"userFeatures\", avgVectorUDF($\"featureList\"))\n",
    "  .select(\"user_id\", \"userFeatures\")\n",
    "\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import breeze.linalg.{DenseVector => BreezeDenseVector, norm}\n",
    "\n",
    "val cosineSimilarity = udf { (vec1: Vector, vec2: Vector) =>\n",
    "  val v1 = BreezeDenseVector(vec1.toArray)\n",
    "  val v2 = BreezeDenseVector(vec2.toArray)\n",
    "  val dot = v1.dot(v2)\n",
    "  val normProduct = norm(v1) * norm(v2)\n",
    "  if (normProduct == 0.0) 0.0 else dot / normProduct\n",
    "}\n",
    "\n",
    "val cbRecs = itemsFeaturized\n",
    "  .crossJoin(userProfile)\n",
    "  .withColumn(\"score\", cosineSimilarity($\"features\", $\"userFeatures\"))\n",
    "  .select(\"user_id\", \"track_id\", \"track_name\", \"score\")\n",
    "  .orderBy($\"user_id\", $\"score\".desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6502cb4-8bcf-46a1-8c29-e8c9158a0ceb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "scored.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b412fc3d-4c13-4f2f-a8e1-3179ad63aafd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val alsRecs = alsModel.recommendForAllUsers(10)\n",
    "  .withColumn(\"rec\", explode($\"recommendations\"))\n",
    "  .select($\"userId\", $\"rec.itemId\".as(\"itemId\"), $\"rec.rating\".as(\"als_score\"))\n",
    "\n",
    "val alpha = 0.7  // weight for ALS; (1 - alpha) for CB\n",
    "\n",
    "val hybrid = alsRecs.join(cbRecs, Seq(\"userId\", \"itemId\"), \"outer\")\n",
    "  .na.fill(0.0, Seq(\"als_score\", \"cb_score\"))\n",
    "  .withColumn(\"hybrid_score\", $\"als_score\" * alpha + $\"cb_score\" * (1 - alpha))\n",
    "  .join(items.select(\"itemId\", \"title\"), \"itemId\")\n",
    "  .orderBy($\"userId\", $\"hybrid_score\".desc)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Recommendation model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
